{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def fd_histogram(image, mask=None):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist  = cv2.calcHist([image],[0,1,2],None,[bins,bins,bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist,hist)\n",
    "\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def fd_hu_moments(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mahotas\n",
    "def fd_haralick(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    haralic = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    \n",
    "    return haralic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def fd_sift(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d_SIFT.create()\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def fd_surf(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d_SURF.create()\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def fd_sift_histogram(image, sift_cl_centers, fixed_size):\n",
    "    \n",
    "    image = cv2.resize(image,fixed_size)\n",
    "    kp, des = fd_sift(image)\n",
    "    closest_cl = []\n",
    "\n",
    "    for i in range(len(des)):\n",
    "        dist = np.sum(np.power(sift_cl_centers - des[i], 2), axis = 1)\n",
    "        nearest_cluster = np.argmin(dist)\n",
    "        closest_cl.append(nearest_cluster)\n",
    "    \n",
    "    counts = np.array([], int)\n",
    "    for j in range(len(sift_cl_centers)):\n",
    "        counts = np.append(counts, closest_cl.count(j))\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def fd_surf_histogram(image, surf_cl_centers, fixed_size):\n",
    "    \n",
    "    image = cv2.resize(image,fixed_size)\n",
    "    kp, des = fd_surf(image)\n",
    "    closest_cl = []\n",
    "\n",
    "    for i in range(len(des)):\n",
    "        dist = np.sum(np.power(surf_cl_centers - des[i], 2), axis = 1)\n",
    "        nearest_cluster = np.argmin(dist)\n",
    "        closest_cl.append(nearest_cluster)\n",
    "    \n",
    "    counts = np.array([], int)\n",
    "    for j in range(len(surf_cl_centers)):\n",
    "        counts = np.append(counts, closest_cl.count(j))\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(df, test_percent):\n",
    "    \n",
    "    test_size = len(df)*(test_percent)/100\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size)\n",
    "    \n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    test_indices = random.sample(indices, test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_feature(df):\n",
    "    \n",
    "    threshold = round(0.01*len(df))\n",
    "    feature_type_list = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        unique_classes = df[column].unique()\n",
    "        example_class = unique_classes[0]\n",
    "            \n",
    "        if (isinstance(example_class, str)) or len(unique_classes) < threshold :\n",
    "            feature_type_list.append(\"Catagorical\")\n",
    "        else:\n",
    "            feature_type_list.append(\"Continuous\")\n",
    "            \n",
    "    return feature_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_potential_splits(df, random_subspace):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    feature_type_list = type_of_feature(df)\n",
    "    _, n_columns = df.shape\n",
    "    \n",
    "    column_indices = list(range(n_columns - 1))\n",
    "\n",
    "    if random_subspace & random_subspace <= len(column_indices):\n",
    "        column_indices =  random.sample(column_indices, random_subspace)\n",
    "    \n",
    "    for i in column_indices:\n",
    "        potential_splits[i] = []\n",
    "        unique_values = df.iloc[:,i].unique()\n",
    "        potential_splits[i] = unique_values\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def check_purity(data):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    label = data[:,-1]\n",
    "    unique_classes = np.unique(label)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def classify_data(data):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    label = data[:,-1]\n",
    "    unique_classes,count_unique_classes = np.unique(label,return_counts=True)\n",
    "\n",
    "    index = count_unique_classes.argmax()  \n",
    "    classification = unique_classes[index]\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_data(data,split_column,split_vlaue, df):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    feature_type_list = type_of_feature(df)\n",
    "    \n",
    "    if feature_type_list[split_column] == 'Continuous':\n",
    "        \n",
    "        data_below = data[data[:,split_column] <= split_vlaue]\n",
    "        data_above = data[data[:,split_column] > split_vlaue]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        data_below = data[data[:,split_column] == split_vlaue]\n",
    "        data_above = data[data[:,split_column] != split_vlaue]\n",
    "        \n",
    "    return data_below,data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    label = data[:,-1]\n",
    "    _,counts = np.unique(label,return_counts=True)\n",
    "\n",
    "    probabilities = counts/counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_overall_entropy(data_below,data_above):\n",
    "    \n",
    "    n_data_points = len(data_below)+len(data_above)\n",
    "    n_data_below = len(data_below)/n_data_points\n",
    "    n_data_above = len(data_above)/n_data_points\n",
    "\n",
    "    overall_entropy = (n_data_below*calculate_entropy(data_below)) + (n_data_above*calculate_entropy(data_above))\n",
    "\n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def best_splits(data, random_subspace, train_df, df):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    overall_entropy = float('inf')\n",
    "    potential_splits = get_potential_splits(train_df, random_subspace)\n",
    "  \n",
    "    for i in potential_splits:\n",
    "        for j in potential_splits[i]:\n",
    "            data_below,data_above = split_data(data,i,j,df)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below,data_above)\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = i\n",
    "                best_split_value = j\n",
    "\n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def bootstrapping(train_df, n_bootstrap):\n",
    "    \n",
    "    bootstrap_indices = np.random.randint(0, len(train_df), n_bootstrap)\n",
    "    df_bootstraped = train_df.iloc[bootstrap_indices]\n",
    "    \n",
    "    return df_bootstraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def decision_tree_algorithm(data, random_subspace, max_depth, train_df, df, counter=0, min_samples = 10):\n",
    "    \n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.array(data)\n",
    "    \n",
    "    if check_purity(data) or (len(data) <= min_samples) or (counter == max_depth):\n",
    "        return classify_data(data)\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        split_column,split_value = best_splits(data, random_subspace, train_df, df)\n",
    "        data_below,data_above = split_data(data,split_column,split_value, df)\n",
    "        \n",
    "        feature_type_list = type_of_feature(df)\n",
    "    \n",
    "        if feature_type_list[split_column] == 'Continuous':\n",
    "            question = f'{df.columns[split_column]} <= {split_value}'\n",
    "        else:\n",
    "            question = f'{df.columns[split_column]} = {split_value}'\n",
    "            \n",
    "        sub_tree = {question : []}\n",
    "        \n",
    "        yes_answer = decision_tree_algorithm(data_below, random_subspace, max_depth, train_df, df, counter)\n",
    "        no_answer = decision_tree_algorithm(data_above, random_subspace, max_depth, train_df, df, counter)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(example, tree): \n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature, compare, value = question.split()\n",
    "    \n",
    "    if compare == '<=':\n",
    "        if example[float(feature)] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    else:\n",
    "        if example[float(feature)] == float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "        \n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_sample(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from p_tqdm import p_map\n",
    "\n",
    "def random_forest_generation(train_df, n_trees, n_bootstrap, n_features, dt_max_depth, df):\n",
    "    \n",
    "    bootstrap_data = []\n",
    "    for i in range(n_trees):\n",
    "        df_bootstraped = bootstrapping(train_df, n_bootstrap)\n",
    "        bootstrap_data.append(df_bootstraped.values)\n",
    "           \n",
    "    n_processor = multiprocessing.cpu_count()\n",
    "    p = Pool(n_processor)\n",
    "    forest = p_map(partial(decision_tree_algorithm,random_subspace=n_features,max_depth=dt_max_depth,train_df=train_df,df=df), bootstrap_data)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_predictions(df, tree):\n",
    "    \n",
    "    if len(df) != 0:\n",
    "    \n",
    "        prediction = []\n",
    "        for i in range(len(df)):\n",
    "            predicted_class = classify_sample(df.iloc[i], tree)\n",
    "            prediction.append(predicted_class)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def random_forest_prediction(df,forest):\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        \n",
    "        df_predictions = {}\n",
    "        for i in range(len(forest)):\n",
    "            name = f'tree-{i}'\n",
    "            prediction = decision_tree_predictions(df, forest[i])\n",
    "            df_predictions[name] = prediction\n",
    "\n",
    "        df_prediction = pd.DataFrame(df_predictions)\n",
    "        random_forest_prediction = df_prediction.mode(axis = 1)[0]\n",
    "    \n",
    "    return random_forest_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, labels):\n",
    "    \n",
    "    if len(predictions) != 0:\n",
    "    \n",
    "        predictions.index = labels.index\n",
    "        predictions_correct = predictions == labels\n",
    "        accuracy = predictions_correct.mean()\n",
    "\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
